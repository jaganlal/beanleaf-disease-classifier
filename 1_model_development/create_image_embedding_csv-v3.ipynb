{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "from imgbeddings import imgbeddings\n",
        "\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "files_to_remove = ['.DS_Store']\n",
        "batch_size = 64\n",
        "\n",
        "root_folder = os.path.normpath(os.getcwd() + os.sep + os.pardir)\n",
        "data_folder = '0_data/beanleaf_dataset'\n",
        "beandataset_folder = os.path.join(root_folder, data_folder)\n",
        "beanleaf_data_partition_folders = os.listdir(beandataset_folder)\n",
        "beanleaf_data_partition_folders = [i for i in beanleaf_data_partition_folders if i not in files_to_remove]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get files from each folder (test, train & validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_scandir_by_extn(dir, ext):\n",
        "    subfolders, files = [], []\n",
        "\n",
        "    for f in os.scandir(dir):\n",
        "        if f.is_dir():\n",
        "            subfolders.append(f.path)\n",
        "        if f.is_file():\n",
        "            if os.path.splitext(f.name)[1].lower() in ext:\n",
        "                files.append(f.path)\n",
        "\n",
        "    for dir in list(subfolders):\n",
        "        sf, f = run_scandir_by_extn(dir, ext)\n",
        "        subfolders.extend(sf)\n",
        "        files.extend(f)\n",
        "    return subfolders, files\n",
        "\n",
        "def run_scandir_with_exclusion(dir, exclude):\n",
        "    subfolders, files = [], []\n",
        "\n",
        "    for f in os.scandir(dir):\n",
        "        if f.is_dir():\n",
        "            subfolders.append(f.path)\n",
        "        if f.is_file():\n",
        "            if f.name not in exclude:\n",
        "                files.append(f.path)\n",
        "\n",
        "    for dir in list(subfolders):\n",
        "        sf, f = run_scandir_with_exclusion(dir, exclude)\n",
        "        subfolders.extend(sf)\n",
        "        files.extend(f)\n",
        "    return subfolders, files\n",
        "\n",
        "test_dataset_folder = os.path.join(beandataset_folder, beanleaf_data_partition_folders[0])\n",
        "test_subfolders, test_files = run_scandir_with_exclusion(test_dataset_folder, files_to_remove)\n",
        "\n",
        "train_dataset_folder = os.path.join(beandataset_folder, beanleaf_data_partition_folders[1])\n",
        "train_subfolders, train_files = run_scandir_with_exclusion(train_dataset_folder, files_to_remove)\n",
        "\n",
        "validation_dataset_folder = os.path.join(beandataset_folder, beanleaf_data_partition_folders[2])\n",
        "validation_subfolders, validation_files = run_scandir_with_exclusion(validation_dataset_folder, files_to_remove)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jaganlalthoppe/workspace/mlops/azure/beanleaf-disease-classifier/env/lib/python3.9/site-packages/huggingface_hub/file_download.py:588: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "ibed = imgbeddings()\n",
        "\n",
        "#column headers for the csv\n",
        "header = ['name', 'url', 'vector']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create embeddings for test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/jaganlalthoppe/workspace/mlops/azure/beanleaf-disease-classifier/0_data/embeddings'"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings_folder = '0_data/embeddings'\n",
        "embeddings_folder_path = os.path.join(root_folder, embeddings_folder)\n",
        "embeddings_folder_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 64/64 [00:03<00:00, 20.12it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 22.32it/s]\n"
          ]
        }
      ],
      "source": [
        "test_file_path = os.path.join(embeddings_folder_path, 'beanleaf_test.csv')\n",
        "with open(test_file_path, 'w', encoding='UTF8', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "\n",
        "    # write the header\n",
        "    writer.writerow(header)\n",
        "\n",
        "    test_file_count = len(test_files)\n",
        "    chunks = (test_file_count - 1)\n",
        "    for i in range(chunks):\n",
        "        test_embeddings = []\n",
        "        files = test_files[i*batch_size:(i+1)*batch_size]\n",
        "        if len(files):\n",
        "            test_embeddings = ibed.to_embeddings(files)\n",
        "            # Iterate directory\n",
        "            for index in range(len(files)):\n",
        "                data = []\n",
        "                data.append(os.path.basename(files[index]))\n",
        "                data.append(files[index])\n",
        "                data.append(test_embeddings[index])\n",
        "\n",
        "                # write the data\n",
        "                writer.writerow(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create embeddings for train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 64/64 [00:02<00:00, 22.95it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 25.95it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 26.95it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 27.54it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 29.48it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 24.48it/s]\n",
            "100%|██████████| 64/64 [00:03<00:00, 20.08it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 23.03it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 26.21it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 26.94it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 25.30it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 24.09it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 25.53it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 25.52it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 24.59it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 21.89it/s]\n"
          ]
        }
      ],
      "source": [
        "train_file_path = os.path.join(embeddings_folder_path, 'beanleaf_train.csv')\n",
        "with open(train_file_path, 'w', encoding='UTF8', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "\n",
        "    # write the header\n",
        "    writer.writerow(header)\n",
        "\n",
        "    train_file_count = len(train_files)\n",
        "    chunks = (train_file_count - 1)\n",
        "    for i in range(chunks):\n",
        "        train_embeddings = []\n",
        "        files = train_files[i*batch_size:(i+1)*batch_size]\n",
        "        if len(files):\n",
        "            train_embeddings = ibed.to_embeddings(files)\n",
        "            # Iterate directory\n",
        "            for index in range(len(files)):\n",
        "                data = []\n",
        "                data.append(os.path.basename(files[index]))\n",
        "                data.append(files[index])\n",
        "                data.append(train_embeddings[index])\n",
        "\n",
        "                # write the data\n",
        "                writer.writerow(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create embeddings for validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 64/64 [00:02<00:00, 23.23it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 24.54it/s]\n"
          ]
        }
      ],
      "source": [
        "validation_file_path = os.path.join(embeddings_folder_path, 'beanleaf_validation.csv')\n",
        "with open(validation_file_path, 'w', encoding='UTF8', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "\n",
        "    # write the header\n",
        "    writer.writerow(header)\n",
        "\n",
        "    validation_file_count = len(validation_files)\n",
        "    chunks = (validation_file_count - 1)\n",
        "    for i in range(chunks):\n",
        "        validation_embeddings = []\n",
        "        files = validation_files[i*batch_size:(i+1)*batch_size]\n",
        "        if len(files):\n",
        "            validation_embeddings = ibed.to_embeddings(files)\n",
        "            # Iterate directory\n",
        "            for index in range(len(files)):\n",
        "                data = []\n",
        "                data.append(os.path.basename(files[index]))\n",
        "                data.append(files[index])\n",
        "                data.append(validation_embeddings[index])\n",
        "\n",
        "                # write the data\n",
        "                writer.writerow(data)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyN3DDB2KsCqw+9BU+ATcfCD",
      "include_colab_link": true,
      "name": "Leaf Disease Classification - Computer Vision.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "353b3b27b727b6bd6d2ff14a3cfe5fe1f5dd4da0574e2f8e60138a2d59c0439e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from skimage import transform\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from imgbeddings import imgbeddings\n",
        "\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "files_to_remove = ['.DS_Store']\n",
        "batch_size = 64\n",
        "\n",
        "root_folder = os.path.normpath(os.getcwd() + os.sep + os.pardir)\n",
        "data_folder_path = '0_data/mock_production_log'\n",
        "data_folder = os.path.join(root_folder, data_folder_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get files from each folder (test, train & validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jaganlalthoppe/workspace/mlops/azure/beanleaf-disease-classifier/env/lib/python3.9/site-packages/huggingface_hub/file_download.py:588: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "ibed = imgbeddings()\n",
        "\n",
        "#column headers for the csv\n",
        "header = ['name', 'url', 'predicted_label', 'score', 'prediction_ts', 'vector']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the model to log the predictions score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-10-07 14:51:13.535271: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "classes = ['angular_leaf_spot', 'bean_rust', 'healthy']\n",
        "model_loaded = tf.keras.models.load_model('./models/')\n",
        "\n",
        "def process(filename):\n",
        "   np_image = Image.open(filename)\n",
        "   np_image = np.array(np_image).astype('float32')\n",
        "   np_image = transform.resize(np_image, (224, 224, 3))\n",
        "   np_image = np.expand_dims(np_image, axis=0)\n",
        "   return np_image\n",
        "\n",
        "def get_predicted_class_and_score(url):\n",
        "   predicted = model_loaded.predict(process(url))\n",
        "   predicted_index = np.argmax(predicted)\n",
        "\n",
        "   return [classes[predicted_index], predicted[0][predicted_index]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create embeddings for test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_scandir_with_exclusion(dir, exclude):\n",
        "    subfolders, files = [], []\n",
        "\n",
        "    for f in os.scandir(dir):\n",
        "        if f.is_dir():\n",
        "            subfolders.append(f.path)\n",
        "        if f.is_file():\n",
        "            if f.name not in exclude:\n",
        "                files.append(f.path)\n",
        "\n",
        "    for dir in list(subfolders):\n",
        "        sf, f = run_scandir_with_exclusion(dir, exclude)\n",
        "        subfolders.extend(sf)\n",
        "        files.extend(f)\n",
        "    return subfolders, files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings_folder = '0_data/embeddings'\n",
        "embeddings_folder_path = os.path.join(root_folder, embeddings_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings_for_all_images = []\n",
        "labels_for_all_images = []\n",
        "\n",
        "def write_to_csv(files, writer, prediction_ts):\n",
        "    file_count = len(files)\n",
        "    chunks = (file_count - 1)\n",
        "    for i in range(chunks):\n",
        "        embeddings = []\n",
        "        batch_files = files[i*batch_size:(i+1)*batch_size]\n",
        "        if len(batch_files):\n",
        "            embeddings = ibed.to_embeddings(batch_files)\n",
        "            # Iterate directory\n",
        "            for index in range(len(batch_files)):\n",
        "                data = []\n",
        "                data.append(os.path.basename(batch_files[index]))\n",
        "                data.append(batch_files[index])\n",
        "                [predicted_class, score] = get_predicted_class_and_score(batch_files[index])\n",
        "                predicted_label = predicted_class\n",
        "\n",
        "                data.append(predicted_label)\n",
        "                data.append(score)\n",
        "                data.append(prediction_ts)\n",
        "                data.append(embeddings[index])\n",
        "\n",
        "                embeddings_for_all_images.append(embeddings[index])\n",
        "                labels_for_all_images.append(predicted_label)\n",
        "\n",
        "                # write the data\n",
        "                writer.writerow(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create embeddings for production (mock) dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "now_ts = datetime.timestamp(datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prod_file_path = os.path.join(embeddings_folder_path, 'beanleaf_production.csv')\n",
        "# image_list = os.listdir(data_folder)\n",
        "# image_list = [i for i in image_list if i not in files_to_remove]\n",
        "prod_subfolders, prod_files = run_scandir_with_exclusion(data_folder, files_to_remove)\n",
        "\n",
        "with open(prod_file_path, 'w', encoding='UTF8', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    # write the header\n",
        "    writer.writerow(header)\n",
        "    write_to_csv(prod_files, writer, now_ts)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyN3DDB2KsCqw+9BU+ATcfCD",
      "include_colab_link": true,
      "name": "Leaf Disease Classification - Computer Vision.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "353b3b27b727b6bd6d2ff14a3cfe5fe1f5dd4da0574e2f8e60138a2d59c0439e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

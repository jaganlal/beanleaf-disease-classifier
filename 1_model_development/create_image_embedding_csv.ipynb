{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "from imgbeddings import imgbeddings\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "files_to_remove = ['.DS_Store']\n",
        "batch_size = 64\n",
        "\n",
        "root_folder = os.path.normpath(os.getcwd() + os.sep + os.pardir)\n",
        "data_folder = '0_data/beanleaf_dataset'\n",
        "beandataset_folder = os.path.join(root_folder, data_folder)\n",
        "beanleaf_data_partition_folders = os.listdir(beandataset_folder)\n",
        "beanleaf_data_partition_folders = [i for i in beanleaf_data_partition_folders if i not in files_to_remove]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get files from each folder (test, train & validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_scandir_by_extn(dir, ext):\n",
        "    subfolders, files = [], []\n",
        "\n",
        "    for f in os.scandir(dir):\n",
        "        if f.is_dir():\n",
        "            subfolders.append(f.path)\n",
        "        if f.is_file():\n",
        "            if os.path.splitext(f.name)[1].lower() in ext:\n",
        "                files.append(f.path)\n",
        "\n",
        "    for dir in list(subfolders):\n",
        "        sf, f = run_scandir_by_extn(dir, ext)\n",
        "        subfolders.extend(sf)\n",
        "        files.extend(f)\n",
        "    return subfolders, files\n",
        "\n",
        "def run_scandir_with_exclusion(dir, exclude):\n",
        "    subfolders, files = [], []\n",
        "\n",
        "    for f in os.scandir(dir):\n",
        "        if f.is_dir():\n",
        "            subfolders.append(f.path)\n",
        "        if f.is_file():\n",
        "            if f.name not in exclude:\n",
        "                files.append(f.path)\n",
        "\n",
        "    for dir in list(subfolders):\n",
        "        sf, f = run_scandir_with_exclusion(dir, exclude)\n",
        "        subfolders.extend(sf)\n",
        "        files.extend(f)\n",
        "    return subfolders, files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jaganlalthoppe/workspace/mlops/azure/beanleaf-disease-classifier/env/lib/python3.9/site-packages/huggingface_hub/file_download.py:588: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "ibed = imgbeddings()\n",
        "\n",
        "#column headers for the csv\n",
        "header = ['name', 'url', 'actual_label', 'predicted_label', 'prediction_ts', 'vector']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create embeddings for test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings_folder = '0_data/embeddings'\n",
        "embeddings_folder_path = os.path.join(root_folder, embeddings_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_dict = {\n",
        "    'angular_leaf_spot': 0,\n",
        "    'bean_rust': 1,\n",
        "    'healthy': 2\n",
        "}\n",
        "\n",
        "def write_to_csv(files, writer, actual_label, predicted_label, prediction_ts, random_prediction=False):\n",
        "    file_count = len(files)\n",
        "    chunks = (file_count - 1)\n",
        "    actual_label = class_dict[actual_label]\n",
        "    predicted_label = class_dict[predicted_label]\n",
        "    classes = ['0', '1', '2']\n",
        "    for i in range(chunks):\n",
        "        embeddings = []\n",
        "        batch_files = files[i*batch_size:(i+1)*batch_size]\n",
        "        if len(batch_files):\n",
        "            embeddings = ibed.to_embeddings(batch_files)\n",
        "            # Iterate directory\n",
        "            for index in range(len(batch_files)):\n",
        "                data = []\n",
        "                data.append(os.path.basename(batch_files[index]))\n",
        "                data.append(batch_files[index])\n",
        "                data.append(actual_label)\n",
        "\n",
        "                if random_prediction:\n",
        "                    predicted_label = random.choice(classes)\n",
        "\n",
        "                data.append(predicted_label)\n",
        "                data.append(prediction_ts)\n",
        "                data.append(embeddings[index])\n",
        "\n",
        "                # write the data\n",
        "                writer.writerow(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create embeddings for train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "now_ts = datetime.timestamp(datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 64/64 [00:02<00:00, 26.59it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 24.90it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 28.07it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 29.81it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 27.50it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 26.25it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 30.42it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 27.45it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 24.53it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 27.31it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 26.70it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 27.87it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 30.75it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 27.41it/s]\n",
            "100%|██████████| 64/64 [00:02<00:00, 28.58it/s]\n"
          ]
        }
      ],
      "source": [
        "train_file_path = os.path.join(embeddings_folder_path, 'beanleaf_train.csv')\n",
        "\n",
        "train_dataset_folder = os.path.join(beandataset_folder, beanleaf_data_partition_folders[1])\n",
        "train_dataset_classes_folder = os.listdir(train_dataset_folder)\n",
        "train_dataset_classes_folder = [i for i in train_dataset_classes_folder if i not in files_to_remove]\n",
        "\n",
        "for train_class in train_dataset_classes_folder:\n",
        "    fullpath_with_class = os.path.join(train_dataset_folder, train_class)\n",
        "    train_subfolders, train_files = run_scandir_with_exclusion(fullpath_with_class, files_to_remove)\n",
        "\n",
        "    with open(train_file_path, 'w', encoding='UTF8', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        # write the header\n",
        "        writer.writerow(header)\n",
        "        write_to_csv(train_files, writer, train_class, train_class, now_ts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create embeddings for validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "validation_file_path = os.path.join(embeddings_folder_path, 'beanleaf_validation.csv')\n",
        "\n",
        "validation_dataset_folder = os.path.join(beandataset_folder, beanleaf_data_partition_folders[2])\n",
        "validation_dataset_classes_folder = os.listdir(validation_dataset_folder)\n",
        "validation_dataset_classes_folder = [i for i in validation_dataset_classes_folder if i not in files_to_remove]\n",
        "\n",
        "for validation_class in validation_dataset_classes_folder:\n",
        "    fullpath_with_class = os.path.join(validation_dataset_folder, validation_class)\n",
        "    validation_subfolders, validation_files = run_scandir_with_exclusion(fullpath_with_class, files_to_remove)\n",
        "\n",
        "    with open(validation_file_path, 'w', encoding='UTF8', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        # write the header\n",
        "        writer.writerow(header)\n",
        "        write_to_csv(validation_files, writer, validation_class, validation_class, now_ts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create embeddings for test dataset (consider as production dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "now_ts = datetime.timestamp(datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_file_path = os.path.join(embeddings_folder_path, 'beanleaf_test.csv')\n",
        "\n",
        "test_dataset_folder = os.path.join(beandataset_folder, beanleaf_data_partition_folders[0])\n",
        "test_dataset_classes_folder = os.listdir(test_dataset_folder)\n",
        "test_dataset_classes_folder = [i for i in test_dataset_classes_folder if i not in files_to_remove]\n",
        "\n",
        "now_ts = datetime.timestamp(datetime.now())\n",
        "for test_class in test_dataset_classes_folder:\n",
        "    fullpath_with_class = os.path.join(test_dataset_folder, test_class)\n",
        "    test_subfolders, test_files = run_scandir_with_exclusion(fullpath_with_class, files_to_remove)\n",
        "    \n",
        "    with open(test_file_path, 'w', encoding='UTF8', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        # write the header\n",
        "        writer.writerow(header)\n",
        "        write_to_csv(test_files, writer, test_class, test_class, now_ts, random_prediction=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyN3DDB2KsCqw+9BU+ATcfCD",
      "include_colab_link": true,
      "name": "Leaf Disease Classification - Computer Vision.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "353b3b27b727b6bd6d2ff14a3cfe5fe1f5dd4da0574e2f8e60138a2d59c0439e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
